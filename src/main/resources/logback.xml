<?xml version="1.0" encoding="UTF-8"?>
<configuration scan="true" scanPeriod="30 seconds">
    <include resource="org/springframework/boot/logging/logback/defaults.xml"/>
    <property name="SERVICE_NAME" value="CARD SERVICES"/>
    <property name="LOG_LEVEL" value="${LOG_LEVEL:-INFO}"/>
    <property name="LOG_PATTERN" value="%d{yyyy-MM-dd HH:mm:ss.SSS} %X{MDCFilter.UUID} %5level ${PID:- } --- [%t] %-40.40logger{40} : %m%n${LOG_EXCEPTION_CONVERSION_WORD:-%wEx}"/>
    <property name="CHARSET" value="UTF8"/>
    <property name="LOG_FILE" value="logs/${SERVICE_NAME}.log"/>
    <property name="LOG_INDEX_FILE" value="logs/${SERVICE_NAME}"/>
    <property name="KAFKA_BOOTSTRAP_SERVERS" value="${LOG_KAFKA_BOOTSTRAP_SERVERS:-172.26.40.102:9092,172.35.14.143:9092,172.35.14.167:9092,172.35.14.179:9092}"/>
    <property name="KAFKA_LOG_TOPIC" value="bifrost-logs"/>
    <appender name="CONSOLE" class="ch.qos.logback.core.ConsoleAppender">
        <target>System.out</target>
        <filter class="ch.qos.logback.classic.filter.ThresholdFilter">
            <level>${LOG_LEVEL}</level>
        </filter>
        <encoder>
            <pattern>${LOG_PATTERN}</pattern>
            <charset>${CHARSET}</charset>
        </encoder>
    </appender>

    <!-- <appender name="FILE" class="net.logstash.logback.appender.LoggingEventAsyncDisruptorAppender"> -->
   <appender name="FILE" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <file>${LOG_FILE}</file>
        <append>true</append>
        <rollingPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy">
            <maxHistory>512</maxHistory>
            <fileNamePattern>${LOG_INDEX_FILE}-%d{yyyy-MM-dd}%i.log</fileNamePattern>
            <maxFileSize>10000KB</maxFileSize>
        </rollingPolicy>
        <encoder>
            <pattern>${LOG_PATTERN}</pattern>
            <charset>${CHARSET}</charset>
        </encoder>
    </appender>

    <appender name="KAFKA" class="com.github.danielwegener.logback.kafka.KafkaAppender">
        <topic>${KAFKA_LOG_TOPIC}</topic>
        <keyingStrategy class="com.github.danielwegener.logback.kafka.keying.NoKeyKeyingStrategy"/>
        <deliveryStrategy class="com.github.danielwegener.logback.kafka.delivery.AsynchronousDeliveryStrategy"/>
        <!--<deliveryStrategy class="com.github.danielwegener.logback.kafka.delivery.BlockingDeliveryStrategy">
            <timeout>0</timeout>
        </deliveryStrategy>-->
        <producerConfig>bootstrap.servers=${KAFKA_BOOTSTRAP_SERVERS}</producerConfig>
        <producerConfig>acks=0</producerConfig>
        <producerConfig>linger.ms=1000</producerConfig>
        <producerConfig>max.block.ms=1000</producerConfig>
        <producerConfig>client.id=${SERVICE_NAME}</producerConfig>
        <producerConfig>compression.type=gzip</producerConfig>
        <!-- this is the fallback appender if kafka is not available. -->
        <appender-ref ref="FILE"/>
        <!--<encoder class="ch.qos.logback.classic.encoder.PatternLayoutEncoder">
            <pattern>
                {
                "severity": "%level",
                "service": "${SERVICE_NAME}",
                "uuid" : "%{MDCFilter.UUID}",
                "pid": "${PID:-}",
                "thread": "%thread",
                "class": "%logger{40}",
                "message": "%message %replace(%exception){'\n','\u2028'}%nopex%n"
                }
            </pattern>
        </encoder>-->
        <encoder>
            <pattern>${LOG_PATTERN}</pattern>
            <charset>${CHARSET}</charset>
        </encoder>
    </appender>

    <root level="${LOG_LEVEL}">
        <appender-ref ref="CONSOLE"/>
        <appender-ref ref="KAFKA"/>
        <appender-ref ref="FILE"/>
    </root>
</configuration>
